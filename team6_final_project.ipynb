{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vsKjuyh-PT2d"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "import torch.optim as optim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "48000\n",
      "12000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hype-parameters\n",
    "num_epochs = 4 # how many times we are running \n",
    "batch_size = 32 # \n",
    "learning_rate = .001 # \n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "                    [\n",
    "                    transforms.ToPILImage(),\n",
    "                    transforms.RandomRotation(30),\n",
    "                    transforms.RandomAffine(degrees=20, translate=(0.1,0.1), scale=(0.9, 1.1)),\n",
    "                    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                    ])\n",
    "\n",
    "mnist = datasets.MNIST(\n",
    "    root='./data', # where to store data\n",
    "    train=True, # tell the code it is training data\n",
    "    download=True, # download the data\n",
    "    transform=train_transform # transform dataset to tensor directly (no preprocessing)\n",
    ") # import data\n",
    "print(len(mnist))\n",
    "\n",
    "mnist_train, mnist_test = torch.utils.data.random_split(mnist, [0.8, .2]) # split data 80/20\n",
    "print(len(mnist_train))\n",
    "print(len(mnist_test))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# init dataloaders to load batches into model\n",
    "train_dl  = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True) \n",
    "\n",
    "test_dl   = torch.utils.data.DataLoader(mnist_test, batch_size=10000, shuffle=False ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to measure accuracy, confusion matrix, precision, recall ,f1 (metrics to measure classification)\n",
    "def print_metrics_function(y_test, y_pred):\n",
    "    print('Accuracy: %.6f' % accuracy_score(y_test, y_pred))\n",
    "    confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confmat)\n",
    "    print('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred, average='weighted'))\n",
    "    print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred, average='weighted'))\n",
    "    f1_measure = f1_score(y_true=y_test, y_pred=y_pred, average='weighted')\n",
    "    print('F1-mesure: %.3f' % f1_measure)\n",
    "    return f1_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier6(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            ## Convolitional Layer 1\n",
    "                nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=1), # 1 input 16 filters, padding so same dim\n",
    "                nn.ReLU(), # ReLU introduce non-linearity\n",
    "                nn.MaxPool2d(2, 2), #pool\n",
    " \n",
    "                ## Convolutional Layer 2\n",
    "                nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=1), # 16 inputs 32 filters\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2, 2),   \n",
    " \n",
    "                ## feed forward layer w/ 1024 neurons, regular layer\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(800, 1024),    ## see how to get 800 below on last cell\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Linear(1024, 10), # maps to output w/ 10 classes\n",
    "                nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "   \n",
    "    def forward(self, inputs):\n",
    "        return self.model(inputs)\n",
    "        \n",
    "def training_loop( num_epochs, model, loss_fn, opt):\n",
    "\n",
    "    losses_list = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for xb, yb in train_dl:\n",
    "            \n",
    "            ## print( xb.shape )   ## check this comes out as [N, 1, 28, 28]\n",
    "            ## yb = torch.squeeze(yb, dim=1)\n",
    "            \n",
    "            y_pred = model(xb)\n",
    "            loss   = loss_fn(y_pred, yb)\n",
    "    \n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "        if epoch % 1 == 0:\n",
    "            print(epoch, \"loss=\", loss)\n",
    "            losses_list.append(  loss  )\n",
    "            \n",
    "    return losses_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss= tensor(0.0903, grad_fn=<NllLossBackward0>)\n",
      "1 loss= tensor(0.0620, grad_fn=<NllLossBackward0>)\n",
      "2 loss= tensor(0.0004, grad_fn=<NllLossBackward0>)\n",
      "3 loss= tensor(0.0150, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = Classifier6() # create our model\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr = learning_rate) # optimizer that does steps\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() # type of loss func\n",
    "\n",
    "my_losses_list = training_loop(num_epochs, model, loss_fn, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch 1/10, Train Loss: 0.1384\n",
      "Epoch 2/10, Train Loss: 0.0430\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 65\u001b[0m\n\u001b[0;32m     62\u001b[0m     avg_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(fold_results) \u001b[38;5;241m/\u001b[39m num_folds\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Validation Accuracy over \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_folds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m folds: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_accuracy\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m k_fold_valid(\n\u001b[0;32m     66\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m Classifier6, \n\u001b[0;32m     67\u001b[0m     train_dataloader \u001b[38;5;241m=\u001b[39m mnist_train,\n\u001b[0;32m     68\u001b[0m     num_folds \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m     69\u001b[0m     num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     70\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m batch_size\n\u001b[0;32m     71\u001b[0m )\n",
      "Cell \u001b[1;32mIn[7], line 35\u001b[0m, in \u001b[0;36mk_fold_valid\u001b[1;34m(model_class, train_dataloader, num_folds, num_epochs, batch_size)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     34\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 35\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     36\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     37\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\mars2\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mars2\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[5], line 27\u001b[0m, in \u001b[0;36mClassifier6.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(inputs)\n",
      "File \u001b[1;32mc:\\Users\\mars2\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mars2\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mars2\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mars2\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mars2\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mars2\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:213\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[1;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mmax_pool2d(\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size,\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding,\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation,\n\u001b[0;32m    219\u001b[0m         ceil_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mceil_mode,\n\u001b[0;32m    220\u001b[0m         return_indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_indices,\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\mars2\\anaconda3\\Lib\\site-packages\\torch\\_jit_internal.py:624\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_false(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mars2\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:830\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[1;32m--> 830\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmax_pool2d(\u001b[38;5;28minput\u001b[39m, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "def k_fold_valid(model_class, train_dataloader, num_folds=5, num_epochs=10, batch_size=32):\n",
    "    kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "    \n",
    "    # Placeholder to store results from each fold\n",
    "    fold_results = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(train_dataloader.dataset)):\n",
    "        print(f\"Fold {fold + 1}/{num_folds}\")\n",
    "\n",
    "        # Split the DataLoader dataset into training and validation sets for this fold\n",
    "        # Create subsets for training and validation by selecting indices from the DataLoader\n",
    "        train_subset = torch.utils.data.Subset(train_dataloader.dataset, train_idx)\n",
    "        val_subset = torch.utils.data.Subset(train_dataloader.dataset, val_idx)\n",
    "\n",
    "        # Create new DataLoaders for each fold\n",
    "        train_loader = torch.utils.data.DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = torch.utils.data.DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # Initialize the model and optimizer\n",
    "        model = model_class()  # Instantiate the model class\n",
    "        criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)  # Optimizer\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()  # Set the model to training mode\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            avg_train_loss = running_loss / len(train_loader)\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_accuracy = correct / total\n",
    "        print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "        fold_results.append(val_accuracy)\n",
    "\n",
    "    # Calculate and display the average validation accuracy\n",
    "    avg_accuracy = sum(fold_results) / num_folds\n",
    "    print(f\"Average Validation Accuracy over {num_folds} folds: {avg_accuracy * 100:.2f}%\")\n",
    "\n",
    "k_fold_valid(\n",
    "    model_class = Classifier6, \n",
    "    train_dataloader = mnist_train,\n",
    "    num_folds = 5,\n",
    "    num_epochs = 10,\n",
    "    batch_size = batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fold 1/5\n",
    "Epoch 1/10, Train Loss: 0.1478\n",
    "Epoch 2/10, Train Loss: 0.0458\n",
    "Epoch 3/10, Train Loss: 0.0306\n",
    "Epoch 4/10, Train Loss: 0.0226\n",
    "Epoch 5/10, Train Loss: 0.0172\n",
    "Epoch 6/10, Train Loss: 0.0147\n",
    "Epoch 7/10, Train Loss: 0.0110\n",
    "Epoch 8/10, Train Loss: 0.0099\n",
    "Epoch 9/10, Train Loss: 0.0080\n",
    "Epoch 10/10, Train Loss: 0.0079\n",
    "Validation Accuracy: 98.69%\n",
    "Fold 2/5\n",
    "Epoch 1/10, Train Loss: 0.1468\n",
    "Epoch 2/10, Train Loss: 0.0456\n",
    "Epoch 3/10, Train Loss: 0.0303\n",
    "Epoch 4/10, Train Loss: 0.0241\n",
    "Epoch 5/10, Train Loss: 0.0176\n",
    "Epoch 6/10, Train Loss: 0.0164\n",
    "Epoch 7/10, Train Loss: 0.0116\n",
    "Epoch 8/10, Train Loss: 0.0110\n",
    "Epoch 9/10, Train Loss: 0.0097\n",
    "Epoch 10/10, Train Loss: 0.0109\n",
    "Validation Accuracy: 98.97%\n",
    "Fold 3/5\n",
    "Epoch 1/10, Train Loss: 0.1421\n",
    "Epoch 2/10, Train Loss: 0.0443\n",
    "Epoch 3/10, Train Loss: 0.0296\n",
    "Epoch 4/10, Train Loss: 0.0215\n",
    "Epoch 5/10, Train Loss: 0.0171\n",
    "Epoch 6/10, Train Loss: 0.0137\n",
    "Epoch 7/10, Train Loss: 0.0112\n",
    "Epoch 8/10, Train Loss: 0.0097\n",
    "Epoch 9/10, Train Loss: 0.0091\n",
    "Epoch 10/10, Train Loss: 0.0074\n",
    "Validation Accuracy: 98.88%\n",
    "Fold 4/5\n",
    "Epoch 1/10, Train Loss: 0.1438\n",
    "Epoch 2/10, Train Loss: 0.0453\n",
    "Epoch 3/10, Train Loss: 0.0308\n",
    "Epoch 4/10, Train Loss: 0.0228\n",
    "Epoch 5/10, Train Loss: 0.0169\n",
    "Epoch 6/10, Train Loss: 0.0141\n",
    "Epoch 7/10, Train Loss: 0.0123\n",
    "Epoch 8/10, Train Loss: 0.0105\n",
    "Epoch 9/10, Train Loss: 0.0100\n",
    "Epoch 10/10, Train Loss: 0.0073\n",
    "Validation Accuracy: 98.92%\n",
    "Fold 5/5\n",
    "Epoch 1/10, Train Loss: 0.1413\n",
    "Epoch 2/10, Train Loss: 0.0474\n",
    "Epoch 3/10, Train Loss: 0.0307\n",
    "Epoch 4/10, Train Loss: 0.0223\n",
    "Epoch 5/10, Train Loss: 0.0177\n",
    "Epoch 6/10, Train Loss: 0.0142\n",
    "Epoch 7/10, Train Loss: 0.0105\n",
    "Epoch 8/10, Train Loss: 0.0112\n",
    "Epoch 9/10, Train Loss: 0.0076\n",
    "Epoch 10/10, Train Loss: 0.0086\n",
    "Validation Accuracy: 98.46%\n",
    "Average Validation Accuracy over 5 folds: 98.78%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.989300\n",
      "Confusion Matrix:\n",
      "[[ 965    0    1    0    0    0    8    0    0    1]\n",
      " [   0 1096    1    0    0    0    2    2    3    0]\n",
      " [   0    3 1008   12    0    0    0    2    1    0]\n",
      " [   0    0    0 1045    0    3    0    0    0    0]\n",
      " [   0    1    1    0  989    0    2    1    1    1]\n",
      " [   0    1    0    2    0  905    5    1    1    0]\n",
      " [   0    0    0    0    0    0  896    0    0    0]\n",
      " [   0    5    2    1    0    0    0 1041    0    6]\n",
      " [   0    1    0    2    1    0    5    1  973    2]\n",
      " [   2    0    0    1   11    1    1    1    8  975]]\n",
      "Precision: 0.989\n",
      "Recall: 0.989\n",
      "F1-mesure: 0.989\n",
      "Accuracy: 0.985500\n",
      "Confusion Matrix:\n",
      "[[205   0   0   0   0   1   1   0   1   0]\n",
      " [  1 232   1   0   0   0   0   0   0   0]\n",
      " [  0   0 184   3   0   0   1   1   0   0]\n",
      " [  0   0   0 213   0   1   0   0   0   0]\n",
      " [  0   0   0   0 168   0   0   1   0   0]\n",
      " [  0   1   0   2   0 170   1   0   0   1]\n",
      " [  0   0   0   0   0   0 192   0   0   0]\n",
      " [  0   2   0   0   0   0   0 204   1   0]\n",
      " [  0   0   0   1   0   0   2   0 200   0]\n",
      " [  0   0   0   0   3   0   0   2   1 203]]\n",
      "Precision: 0.986\n",
      "Recall: 0.986\n",
      "F1-mesure: 0.985\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): # detach grad tracking for tensors\n",
    "    for x_real, y_real in test_dl:\n",
    "        y_pred = model(  x_real  )\n",
    "        \n",
    "        vals, indeces = torch.max( y_pred, dim=1  )\n",
    "        preds = indeces\n",
    "        print_metrics_function(y_real, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), \"team6_final_weights.pth\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6])\n",
      "tensor([9])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([9])\n",
      "tensor([8])\n",
      "tensor([9])\n",
      "tensor([9])\n",
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "# test handwritten digits\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load the image, replace with custom file path\n",
    "image_path = [\n",
    "    \"0.png\",\n",
    "    \"1.png\",\n",
    "    \"2.png\",\n",
    "    \"3.png\",\n",
    "    \"4.png\",\n",
    "    \"5.png\",\n",
    "    \"6.png\",\n",
    "    \"7.png\",\n",
    "    \"8.png\",\n",
    "    \"9.png\",\n",
    "]  \n",
    "for path in image_path:\n",
    "    image = Image.open(path)\n",
    "\n",
    "    # convert to greyscale & resize\n",
    "    image = image.convert(\"L\")\n",
    "    image = image.resize((28,28))\n",
    "\n",
    "    # convert to tensor\n",
    "    transform = transforms.ToTensor()\n",
    "    image_tr = transform(image)\n",
    "    image_tr = image_tr.unsqueeze(dim=0) # add batch dimension\n",
    "    image_tr = 1 - image_tr\n",
    "\n",
    "    pred = model(image_tr)\n",
    "    vals, indeces = torch.max( pred, dim=1  )\n",
    "    preds = indeces\n",
    "    print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commented out, took out validation idk if we need\n",
    "\n",
    "#from torch.utils.data import DataLoader\n",
    "#validation_loader = DataLoader(mnist_valid, batch_size=64, shuffle=False)\n",
    "\n",
    "# Iterate through the DataLoader to validate\n",
    "#for X_valid, y_valid in validation_loader:\n",
    "    # Use X_valid and y_valid for validation here\n",
    "    #break  # Remove this if processing the entire validation set\n",
    "#y_valid_pred = model(X_valid)  # Predictions on validation set\n",
    "\n",
    "#val_accuracy = accuracy_score(y_valid, y_valid_pred)\n",
    "#print(f\"Validation Accuracy: {val_accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
