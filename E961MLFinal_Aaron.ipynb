{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vsKjuyh-PT2d"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "import torch.optim as optim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "48000\n",
      "12000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hype-parameters\n",
    "num_epochs = 4 # how many times we are running \n",
    "batch_size = 32 # \n",
    "learning_rate = .001 # \n",
    "\n",
    "mnist = datasets.MNIST(\n",
    "    root='./data', # where to store data\n",
    "    train=True, # tell the code it is training data\n",
    "    download=True, # download the data\n",
    "    transform=transforms.ToTensor() # transform dataset to tensor directly (no preprocessing)\n",
    ") # import data\n",
    "print(len(mnist))\n",
    "\n",
    "mnist_train, mnist_test = torch.utils.data.random_split(mnist, [0.8, .2]) # split data 80/20\n",
    "print(len(mnist_train))\n",
    "print(len(mnist_test))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# init dataloaders to load batches into model\n",
    "train_dl  = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True  ) \n",
    "\n",
    "test_dl   = torch.utils.data.DataLoader(mnist_test,  batch_size=10000,      shuffle=False ) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to measure accuracy, confusion matrix, precision, recall ,f1 (metrics to measure classification)\n",
    "def print_metrics_function(y_test, y_pred):\n",
    "    print('Accuracy: %.6f' % accuracy_score(y_test, y_pred))\n",
    "    confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confmat)\n",
    "    print('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred, average='weighted'))\n",
    "    print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred, average='weighted'))\n",
    "    f1_measure = f1_score(y_true=y_test, y_pred=y_pred, average='weighted')\n",
    "    print('F1-mesure: %.3f' % f1_measure)\n",
    "    return f1_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier_CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            ## Convolitional Layer 1\n",
    "                nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=1), # 1 input 16 filters, padding so same dim\n",
    "                nn.ReLU(), # ReLU introduce non-linearity\n",
    "                nn.MaxPool2d(2, 2), #pool\n",
    " \n",
    "                ## Convolutional Layer 2\n",
    "                nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=1), # 16 inputs 32 filters\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2, 2),   \n",
    " \n",
    "                ## feed forward layer w/ 1024 neurons, regular layer\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(800, 1024),    ## see how to get 800 below on last cell\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Linear(1024, 10), # maps to output w/ 10 classes\n",
    "                nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "   \n",
    "    def forward(self, inputs):\n",
    "            \n",
    "        return self.model(inputs)\n",
    "        \n",
    "def training_loop( num_epochs, model, loss_fn, opt):\n",
    "\n",
    "    losses_list = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for xb, yb in train_dl:\n",
    "            \n",
    "            ## print( xb.shape )   ## check this comes out as [N, 1, 28, 28]\n",
    "            ## yb = torch.squeeze(yb, dim=1)\n",
    "            \n",
    "            y_pred = model(xb)\n",
    "            loss   = loss_fn(y_pred, yb)\n",
    "    \n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "        if epoch % 1 == 0:\n",
    "            print(epoch, \"loss=\", loss)\n",
    "            losses_list.append(  loss  )\n",
    "            \n",
    "    return losses_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss= tensor(0.0142, grad_fn=<NllLossBackward0>)\n",
      "1 loss= tensor(0.0030, grad_fn=<NllLossBackward0>)\n",
      "2 loss= tensor(0.1251, grad_fn=<NllLossBackward0>)\n",
      "3 loss= tensor(0.0001, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = Classifier_CNN() # create our model\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr = learning_rate) # optimizer that does steps\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() # type of loss func\n",
    "\n",
    "my_losses_list = training_loop(num_epochs, model, loss_fn, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.987900\n",
      "Confusion Matrix:\n",
      "[[ 963    0    0    0    0    1    0    0    0    2]\n",
      " [   0 1126    1    0    0    0    1    2    0    0]\n",
      " [   2    1  989    1    0    0    0   11    0    0]\n",
      " [   0    0    6  997    0    6    0    9    3    0]\n",
      " [   0    1    1    0  969    0    1    1    1    0]\n",
      " [   1    0    0    0    0  889    0    0    1    0]\n",
      " [   2    0    1    0    4   14  952    0    2    0]\n",
      " [   0    4    0    0    1    0    0 1081    2    1]\n",
      " [   1    2    3    1    2    1    0    3  929    0]\n",
      " [   0    2    0    0    8    4    0    9    1  984]]\n",
      "Precision: 0.988\n",
      "Recall: 0.988\n",
      "F1-mesure: 0.988\n",
      "Accuracy: 0.989000\n",
      "Confusion Matrix:\n",
      "[[181   0   0   0   0   0   0   0   0   0]\n",
      " [  0 200   0   0   0   0   0   1   0   0]\n",
      " [  1   0 199   0   0   0   0   2   1   0]\n",
      " [  0   0   0 194   0   2   0   0   0   1]\n",
      " [  0   0   0   0 202   0   0   0   0   1]\n",
      " [  0   0   0   0   0 187   1   0   1   0]\n",
      " [  0   0   0   0   0   3 188   0   1   0]\n",
      " [  0   1   0   0   0   0   0 222   0   0]\n",
      " [  0   0   0   0   0   2   0   0 213   0]\n",
      " [  0   1   1   0   1   0   0   0   1 192]]\n",
      "Precision: 0.989\n",
      "Recall: 0.989\n",
      "F1-mesure: 0.989\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad(): # detach grad tracking for tensors\n",
    "    for x_real, y_real in test_dl:\n",
    "        \n",
    "        y_pred = model(  x_real  )\n",
    "        \n",
    "        vals, indeces = torch.max( y_pred, dim=1  )\n",
    "        preds = indeces\n",
    "        print_metrics_function(y_real, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Example: k-fold cross-validation with a model\n",
    "#cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commented out, took out validation idk if we need\n",
    "\n",
    "#from torch.utils.data import DataLoader\n",
    "#validation_loader = DataLoader(mnist_valid, batch_size=64, shuffle=False)\n",
    "\n",
    "# Iterate through the DataLoader to validate\n",
    "#for X_valid, y_valid in validation_loader:\n",
    "    # Use X_valid and y_valid for validation here\n",
    "    #break  # Remove this if processing the entire validation set\n",
    "#y_valid_pred = model(X_valid)  # Predictions on validation set\n",
    "\n",
    "#val_accuracy = accuracy_score(y_valid, y_valid_pred)\n",
    "#print(f\"Validation Accuracy: {val_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
