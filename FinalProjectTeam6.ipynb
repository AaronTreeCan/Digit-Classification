{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vsKjuyh-PT2d"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "import torch.optim as optim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "48000\n",
      "12000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hype-parameters\n",
    "num_epochs = 4 # how many times we are running \n",
    "batch_size = 32 # \n",
    "learning_rate = .001 # \n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "                    [\n",
    "                    transforms.RandomRotation(30),\n",
    "                    transforms.RandomAffine(degrees=20, translate=(0.1,0.1), scale=(0.9, 1.1)),\n",
    "                    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                    ])\n",
    "\n",
    "mnist = datasets.MNIST(\n",
    "    root='./data', # where to store data\n",
    "    train=True, # tell the code it is training data\n",
    "    download=True, # download the data\n",
    "    transform=train_transform # transform dataset to tensor directly (no preprocessing)\n",
    ") # import data\n",
    "print(len(mnist))\n",
    "\n",
    "mnist_train, mnist_test = torch.utils.data.random_split(mnist, [0.8, .2]) # split data 80/20\n",
    "print(len(mnist_train))\n",
    "print(len(mnist_test))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# init dataloaders to load batches into model\n",
    "train_dl  = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True) \n",
    "\n",
    "test_dl   = torch.utils.data.DataLoader(mnist_test, batch_size=10000, shuffle=False ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to measure accuracy, confusion matrix, precision, recall ,f1 (metrics to measure classification)\n",
    "def print_metrics_function(y_test, y_pred):\n",
    "    print('Accuracy: %.6f' % accuracy_score(y_test, y_pred))\n",
    "    confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confmat)\n",
    "    print('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred, average='weighted'))\n",
    "    print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred, average='weighted'))\n",
    "    f1_measure = f1_score(y_true=y_test, y_pred=y_pred, average='weighted')\n",
    "    print('F1-mesure: %.3f' % f1_measure)\n",
    "    return f1_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier6(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            ## Convolitional Layer 1\n",
    "                nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=1), # 1 input 16 filters, padding so same dim\n",
    "                nn.ReLU(), # ReLU introduce non-linearity\n",
    "                nn.MaxPool2d(2, 2), #pool\n",
    " \n",
    "                ## Convolutional Layer 2\n",
    "                nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=1), # 16 inputs 32 filters\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2, 2),   \n",
    " \n",
    "                ## feed forward layer w/ 1024 neurons, regular layer\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(800, 1024),    ## see how to get 800 below on last cell\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Linear(1024, 10), # maps to output w/ 10 classes\n",
    "                nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "   \n",
    "    def forward(self, inputs):\n",
    "        return self.model(inputs)\n",
    "        \n",
    "def training_loop( num_epochs, model, loss_fn, opt):\n",
    "\n",
    "    losses_list = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for xb, yb in train_dl:\n",
    "            \n",
    "            ## print( xb.shape )   ## check this comes out as [N, 1, 28, 28]\n",
    "            ## yb = torch.squeeze(yb, dim=1)\n",
    "            \n",
    "            y_pred = model(xb)\n",
    "            loss   = loss_fn(y_pred, yb)\n",
    "    \n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "        if epoch % 1 == 0:\n",
    "            print(epoch, \"loss=\", loss)\n",
    "            losses_list.append(  loss  )\n",
    "            \n",
    "    return losses_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss= tensor(0.1019, grad_fn=<NllLossBackward0>)\n",
      "1 loss= tensor(0.0654, grad_fn=<NllLossBackward0>)\n",
      "2 loss= tensor(0.0083, grad_fn=<NllLossBackward0>)\n",
      "3 loss= tensor(0.0150, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = Classifier6() # create our model\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr = learning_rate) # optimizer that does steps\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() # type of loss func\n",
    "\n",
    "my_losses_list = training_loop(num_epochs, model, loss_fn, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8])\n",
      "tensor([8])\n",
      "tensor([8])\n",
      "tensor([8])\n",
      "tensor([8])\n",
      "tensor([8])\n",
      "tensor([8])\n",
      "tensor([8])\n",
      "tensor([8])\n",
      "tensor([8])\n"
     ]
    }
   ],
   "source": [
    "# test handwritten digits\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load the image, replace with custom file path\n",
    "image_path = [\n",
    "    \"0.png\",\n",
    "    \"1.png\",\n",
    "    \"2.png\",\n",
    "    \"3.png\",\n",
    "    \"4.png\",\n",
    "    \"5.png\",\n",
    "    \"6.png\",\n",
    "    \"7.png\",\n",
    "    \"8.png\",\n",
    "    \"9.png\",\n",
    "]  \n",
    "for path in image_path:\n",
    "    image = Image.open(path)\n",
    "\n",
    "    # convert to greyscale & resize\n",
    "    image = image.convert(\"L\")\n",
    "    image = image.resize((28,28))\n",
    "\n",
    "    # convert to tensor\n",
    "    transform = transforms.ToTensor()\n",
    "    image_tr = transform(image)\n",
    "    image_tr = image_tr.unsqueeze(dim=0) # add batch dimension\n",
    "    image_tr = 1 - image_tr\n",
    "\n",
    "    pred = model(image_tr)\n",
    "    vals, indeces = torch.max( pred, dim=1  )\n",
    "    preds = indeces\n",
    "    print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 65\u001b[0m\n\u001b[0;32m     62\u001b[0m     avg_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(fold_results) \u001b[38;5;241m/\u001b[39m num_folds\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Validation Accuracy over \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_folds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m folds: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_accuracy\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m k_fold_valid(\n\u001b[0;32m     66\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m Classifier6, \n\u001b[0;32m     67\u001b[0m     train_dataloader \u001b[38;5;241m=\u001b[39m mnist_train,\n\u001b[0;32m     68\u001b[0m     num_folds \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m     69\u001b[0m     num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     70\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m batch_size\n\u001b[0;32m     71\u001b[0m )\n",
      "Cell \u001b[1;32mIn[10], line 33\u001b[0m, in \u001b[0;36mk_fold_valid\u001b[1;34m(model_class, train_dataloader, num_folds, num_epochs, batch_size)\u001b[0m\n\u001b[0;32m     31\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# Set the model to training mode\u001b[39;00m\n\u001b[0;32m     32\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     34\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     35\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n",
      "File \u001b[1;32mc:\\Users\\mars2\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\mars2\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\mars2\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\mars2\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32mc:\\Users\\mars2\\anaconda3\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:146\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    143\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\mars2\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\mars2\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mto_tensor(pic)\n",
      "File \u001b[1;32mc:\\Users\\mars2\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\functional.py:176\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    174\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "def k_fold_valid(model_class, train_dataloader, num_folds=5, num_epochs=10, batch_size=32):\n",
    "    kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "    \n",
    "    # Placeholder to store results from each fold\n",
    "    fold_results = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(train_dataloader.dataset)):\n",
    "        print(f\"Fold {fold + 1}/{num_folds}\")\n",
    "\n",
    "        # Split the DataLoader dataset into training and validation sets for this fold\n",
    "        # Create subsets for training and validation by selecting indices from the DataLoader\n",
    "        train_subset = torch.utils.data.Subset(train_dataloader.dataset, train_idx)\n",
    "        val_subset = torch.utils.data.Subset(train_dataloader.dataset, val_idx)\n",
    "\n",
    "        # Create new DataLoaders for each fold\n",
    "        train_loader = torch.utils.data.DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = torch.utils.data.DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # Initialize the model and optimizer\n",
    "        model = model_class()  # Instantiate the model class\n",
    "        criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)  # Optimizer\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()  # Set the model to training mode\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            avg_train_loss = running_loss / len(train_loader)\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_accuracy = correct / total\n",
    "        print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "        fold_results.append(val_accuracy)\n",
    "\n",
    "    # Calculate and display the average validation accuracy\n",
    "    avg_accuracy = sum(fold_results) / num_folds\n",
    "    print(f\"Average Validation Accuracy over {num_folds} folds: {avg_accuracy * 100:.2f}%\")\n",
    "\n",
    "k_fold_valid(\n",
    "    model_class = Classifier6, \n",
    "    train_dataloader = mnist_train,\n",
    "    num_folds = 5,\n",
    "    num_epochs = 10,\n",
    "    batch_size = batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fold 1/5\n",
    "Epoch 1/10, Train Loss: 0.1478\n",
    "Epoch 2/10, Train Loss: 0.0458\n",
    "Epoch 3/10, Train Loss: 0.0306\n",
    "Epoch 4/10, Train Loss: 0.0226\n",
    "Epoch 5/10, Train Loss: 0.0172\n",
    "Epoch 6/10, Train Loss: 0.0147\n",
    "Epoch 7/10, Train Loss: 0.0110\n",
    "Epoch 8/10, Train Loss: 0.0099\n",
    "Epoch 9/10, Train Loss: 0.0080\n",
    "Epoch 10/10, Train Loss: 0.0079\n",
    "Validation Accuracy: 98.69%\n",
    "Fold 2/5\n",
    "Epoch 1/10, Train Loss: 0.1468\n",
    "Epoch 2/10, Train Loss: 0.0456\n",
    "Epoch 3/10, Train Loss: 0.0303\n",
    "Epoch 4/10, Train Loss: 0.0241\n",
    "Epoch 5/10, Train Loss: 0.0176\n",
    "Epoch 6/10, Train Loss: 0.0164\n",
    "Epoch 7/10, Train Loss: 0.0116\n",
    "Epoch 8/10, Train Loss: 0.0110\n",
    "Epoch 9/10, Train Loss: 0.0097\n",
    "Epoch 10/10, Train Loss: 0.0109\n",
    "Validation Accuracy: 98.97%\n",
    "Fold 3/5\n",
    "Epoch 1/10, Train Loss: 0.1421\n",
    "Epoch 2/10, Train Loss: 0.0443\n",
    "Epoch 3/10, Train Loss: 0.0296\n",
    "Epoch 4/10, Train Loss: 0.0215\n",
    "Epoch 5/10, Train Loss: 0.0171\n",
    "Epoch 6/10, Train Loss: 0.0137\n",
    "Epoch 7/10, Train Loss: 0.0112\n",
    "Epoch 8/10, Train Loss: 0.0097\n",
    "Epoch 9/10, Train Loss: 0.0091\n",
    "Epoch 10/10, Train Loss: 0.0074\n",
    "Validation Accuracy: 98.88%\n",
    "Fold 4/5\n",
    "Epoch 1/10, Train Loss: 0.1438\n",
    "Epoch 2/10, Train Loss: 0.0453\n",
    "Epoch 3/10, Train Loss: 0.0308\n",
    "Epoch 4/10, Train Loss: 0.0228\n",
    "Epoch 5/10, Train Loss: 0.0169\n",
    "Epoch 6/10, Train Loss: 0.0141\n",
    "Epoch 7/10, Train Loss: 0.0123\n",
    "Epoch 8/10, Train Loss: 0.0105\n",
    "Epoch 9/10, Train Loss: 0.0100\n",
    "Epoch 10/10, Train Loss: 0.0073\n",
    "Validation Accuracy: 98.92%\n",
    "Fold 5/5\n",
    "Epoch 1/10, Train Loss: 0.1413\n",
    "Epoch 2/10, Train Loss: 0.0474\n",
    "Epoch 3/10, Train Loss: 0.0307\n",
    "Epoch 4/10, Train Loss: 0.0223\n",
    "Epoch 5/10, Train Loss: 0.0177\n",
    "Epoch 6/10, Train Loss: 0.0142\n",
    "Epoch 7/10, Train Loss: 0.0105\n",
    "Epoch 8/10, Train Loss: 0.0112\n",
    "Epoch 9/10, Train Loss: 0.0076\n",
    "Epoch 10/10, Train Loss: 0.0086\n",
    "Validation Accuracy: 98.46%\n",
    "Average Validation Accuracy over 5 folds: 98.78%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.971600\n",
      "Confusion Matrix:\n",
      "[[1007    0    2    1    0    0    4    0    3    1]\n",
      " [   0 1084    3    3    1    0    0    7   11    1]\n",
      " [   1    4  956    3    1    0    0    8   16    2]\n",
      " [   1    0    4  975    1    6    3    2   11    2]\n",
      " [   0    0    8    0  954    0    3    3   12   22]\n",
      " [   0    0    0    4    2  918   10    0   10    3]\n",
      " [   4    1    0    0    1    8  937    0    5    0]\n",
      " [   1    3   10    4    0    0    0  962    2    6]\n",
      " [   2    0    4    0    1    2    6    2  967    4]\n",
      " [   3    0    2    3    6    3    1    5   16  956]]\n",
      "Precision: 0.972\n",
      "Recall: 0.972\n",
      "F1-mesure: 0.972\n",
      "Accuracy: 0.968000\n",
      "Confusion Matrix:\n",
      "[[191   0   1   0   0   0   0   0   2   0]\n",
      " [  0 227   1   1   1   0   0   1   0   0]\n",
      " [  1   0 189   0   1   0   0   4   0   1]\n",
      " [  0   0   1 187   0   0   0   0   2   0]\n",
      " [  1   0   0   0 185   0   1   1   3   9]\n",
      " [  0   0   0   2   0 179   1   0   3   2]\n",
      " [  1   0   0   0   0   1 194   0   1   1]\n",
      " [  0   0   6   1   0   0   0 200   0   0]\n",
      " [  0   1   1   0   1   0   0   0 182   2]\n",
      " [  0   0   0   0   1   1   0   2   4 202]]\n",
      "Precision: 0.969\n",
      "Recall: 0.968\n",
      "F1-mesure: 0.968\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): # detach grad tracking for tensors\n",
    "    for x_real, y_real in test_dl:\n",
    "        y_pred = model(  x_real  )\n",
    "        \n",
    "        vals, indeces = torch.max( y_pred, dim=1  )\n",
    "        preds = indeces\n",
    "        print_metrics_function(y_real, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), \"team6_final_weights.pth\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8])\n",
      "tensor([1])\n",
      "tensor([8])\n",
      "tensor([3])\n",
      "tensor([4])\n",
      "tensor([9])\n",
      "tensor([8])\n",
      "tensor([8])\n",
      "tensor([8])\n",
      "tensor([8])\n"
     ]
    }
   ],
   "source": [
    "# test handwritten digits\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load the image, replace with custom file path\n",
    "image_path = [\n",
    "    \"0.png\",\n",
    "    \"1.png\",\n",
    "    \"2.png\",\n",
    "    \"3.png\",\n",
    "    \"4.png\",\n",
    "    \"5.png\",\n",
    "    \"6.png\",\n",
    "    \"7.png\",\n",
    "    \"8.png\",\n",
    "    \"9.png\",\n",
    "]  \n",
    "for path in image_path:\n",
    "    image = Image.open(path)\n",
    "\n",
    "    # convert to greyscale & resize\n",
    "    image = image.convert(\"L\")\n",
    "    image = image.resize((28,28))\n",
    "\n",
    "    # convert to tensor\n",
    "    transform = transforms.ToTensor()\n",
    "    image_tr = transform(image)\n",
    "    image_tr = image_tr.unsqueeze(dim=0) # add batch dimension\n",
    "    image_tr = 1 - image_tr\n",
    "\n",
    "    pred = model(image_tr)\n",
    "    vals, indeces = torch.max( pred, dim=1  )\n",
    "    preds = indeces\n",
    "    print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commented out, took out validation idk if we need\n",
    "\n",
    "#from torch.utils.data import DataLoader\n",
    "#validation_loader = DataLoader(mnist_valid, batch_size=64, shuffle=False)\n",
    "\n",
    "# Iterate through the DataLoader to validate\n",
    "#for X_valid, y_valid in validation_loader:\n",
    "    # Use X_valid and y_valid for validation here\n",
    "    #break  # Remove this if processing the entire validation set\n",
    "#y_valid_pred = model(X_valid)  # Predictions on validation set\n",
    "\n",
    "#val_accuracy = accuracy_score(y_valid, y_valid_pred)\n",
    "#print(f\"Validation Accuracy: {val_accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
